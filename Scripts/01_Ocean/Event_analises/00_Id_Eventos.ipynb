{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import numpy  as np\n",
    "import scipy\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import AutoMinorLocator, StrMethodFormatter\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from gsw import p_from_z\n",
    "from gsw import pot_rho_t_exact as prho\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "\n",
    "from scipy import interpolate, ndimage\n",
    "from PIL import Image\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorys Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of all .nc files available in different folders\n",
    "filenames = glob.glob(\"d:/00_Masters/01_Dados/Mercator/*.nc\")\n",
    "dsmerged = xr.open_mfdataset(filenames)\n",
    "\n",
    "# # Corrige erro de datas duplicadas pelo mfdataset\n",
    "_, index = np.unique(dsmerged['time'], return_index=True)\n",
    "dsmerged = dsmerged.isel(time=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select area and export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO PRAIA DO FORTE - A0\n",
    "PF = dsmerged.interp(latitude=-12.60288903576, longitude=-37.974646744616, method='quadratic')\n",
    "PF.to_netcdf('d:/00_Masters/01_Dados/Glorys_Station_PF.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO SERGIPE - Canyon Sao Francisco / Canyon do Japaratuba\n",
    "SE = dsmerged.sel(latitude=-11.0, longitude=-36.75, method='nearest')\n",
    "SE.to_netcdf('d:/00_Masters/01_Dados/Glorys_Station_AL_CSF.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO BAIA DE CAMAMU - Canyon Guaibim\n",
    "BAcam = dsmerged.sel(latitude=-13.33, longitude=-38.75, method='nearest')\n",
    "BAcam.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_BA_CAM.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\00_Masters\\00_Work_onGIT\\Scripts\\01_Ocean\\07_Id_Eventos.ipynb Célula: 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/00_Masters/00_Work_onGIT/Scripts/01_Ocean/07_Id_Eventos.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# ESTAÇÃO AL - Maragogigi\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/00_Masters/00_Work_onGIT/Scripts/01_Ocean/07_Id_Eventos.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m AL \u001b[39m=\u001b[39m dsmerged\u001b[39m.\u001b[39msel(latitude\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m9.17\u001b[39m, longitude\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m34.92\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/00_Masters/00_Work_onGIT/Scripts/01_Ocean/07_Id_Eventos.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m AL\u001b[39m.\u001b[39;49mto_netcdf(\u001b[39m'\u001b[39;49m\u001b[39md:/00_Masters/01_Dados/Mercator/Glorys_Station_AL.nc\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\xarray\\core\\dataset.py:1902\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[1;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1899\u001b[0m     encoding \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1900\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_netcdf\n\u001b[1;32m-> 1902\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(\n\u001b[0;32m   1903\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1904\u001b[0m     path,\n\u001b[0;32m   1905\u001b[0m     mode,\n\u001b[0;32m   1906\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[0;32m   1907\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[0;32m   1908\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m   1909\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1910\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[0;32m   1911\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m   1912\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[0;32m   1913\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\xarray\\backends\\api.py:1081\u001b[0m, in \u001b[0;36mto_netcdf\u001b[1;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m multifile:\n\u001b[0;32m   1079\u001b[0m     \u001b[39mreturn\u001b[39;00m writer, store\n\u001b[1;32m-> 1081\u001b[0m writes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39;49msync(compute\u001b[39m=\u001b[39;49mcompute)\n\u001b[0;32m   1083\u001b[0m \u001b[39mif\u001b[39;00m path_or_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1084\u001b[0m     store\u001b[39m.\u001b[39msync()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\xarray\\backends\\common.py:166\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[1;34m(self, compute)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m# TODO: consider wrapping targets with dask.delayed, if this makes\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m# for any discernable difference in perforance, e.g.,\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m# targets = [dask.delayed(t) for t in self.targets]\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m delayed_store \u001b[39m=\u001b[39m da\u001b[39m.\u001b[39;49mstore(\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msources,\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets,\n\u001b[0;32m    169\u001b[0m     lock\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock,\n\u001b[0;32m    170\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[0;32m    171\u001b[0m     flush\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    172\u001b[0m     regions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregions,\n\u001b[0;32m    173\u001b[0m )\n\u001b[0;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msources \u001b[39m=\u001b[39m []\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\array\\core.py:1226\u001b[0m, in \u001b[0;36mstore\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[39melif\u001b[39;00m compute:\n\u001b[0;32m   1225\u001b[0m     store_dsk \u001b[39m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[1;32m-> 1226\u001b[0m     compute_as_if_collection(Array, store_dsk, map_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1227\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\base.py:347\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[1;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m# see https://github.com/dask/dask/issues/8991.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# This merge should be removed once the underlying issue is fixed.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m dsk2 \u001b[39m=\u001b[39m HighLevelGraph\u001b[39m.\u001b[39mmerge(dsk2)\n\u001b[1;32m--> 347\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[1;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[0;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\dask\\local.py:130\u001b[0m, in \u001b[0;36mqueue_get\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m         \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, timeout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    131\u001b[0m     \u001b[39mexcept\u001b[39;00m Empty:\n\u001b[0;32m    132\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ESTAÇÃO AL - Maragogigi\n",
    "AL = dsmerged.sel(latitude=-9.17, longitude=-34.92, method='nearest')\n",
    "AL.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_AL.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO PE - Cabo de Santo Agostinho\n",
    "PE_CSA = dsmerged.sel(latitude=-8.33, longitude=-34.66, method='nearest')\n",
    "PE_CSA.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_PE_CSA.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO PE - Ponta das Pedras / Ilha de Itamaracá\n",
    "PE_2 = dsmerged.sel(latitude=-7.75, longitude=-34.50, method='nearest')\n",
    "PE_2.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_PE_2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO JP\n",
    "JP = dsmerged.sel(latitude=-6.5, longitude=-34.75, method='nearest')\n",
    "JP.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_JP.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTAÇÃO JP\n",
    "JP = dsmerged.sel(latitude=-6.5, longitude=-34.75, method='nearest')\n",
    "JP.to_netcdf('d:/00_Masters/01_Dados/Mercator/Glorys_Station_JP.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_value = 0.2\n",
    "\n",
    "An_list = []\n",
    "dates_list = []\n",
    "\n",
    "for i,n in enumerate(anomaly.index):\n",
    "    if i < (len(anomaly)-1):\n",
    "        a = anomaly.thetao[i+1] - anomaly.thetao[i]\n",
    "        d = anomaly.index[i+1]\n",
    "        if (a < cutoff_value):\n",
    "            An_list.append(a)\n",
    "            dates_list.append(d)\n",
    "        elif (len(dates_list)>1) and (dates_list[-1] == anomaly.index[i]) and (a < 0):\n",
    "            An_list.append(a)\n",
    "            dates_list.append(d)\n",
    "\n",
    "\n",
    "An = pd.DataFrame({'time': dates_list,'diff': An_list})\n",
    "An.set_index('time',inplace=True,drop=True)\n",
    "\n",
    "An.to_csv('D:/Users/julia/Desktop/Academia/01_Mestrado/Resultados/PF-Anomalies_2017-2018.csv',sep=';',decimal=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
